# FaceGeneration
[![Udacity Deep Learning Nanodegree](https://img.shields.io/badge/Udacity-Deep%20Learning%20ND-deepskyblue?style=flat&logo=udacity)](https://www.udacity.com/course/deep-learning-nanodegree--nd101)
[![TensorFlow](https://img.shields.io/badge/%20-TensorFlow-grey?style=flat&logo=tensorflow)](https://www.tensorflow.org/) \
In this project, I designed a DCGAN (Deep Convolutional Generative Adversarial Network) that could generate various images based on the training data. 
The DCGAN model consists two parts: a generator and a discriminator. 
The generator's job is to generate images as close to the ground truth images as possible. 
Then, the discriminator will compare the generated images with ground truth image and tell the generator how to improve.
Because of the discriminator, the DCGAN model can train itself without any labeled data (unsupervised training).
The model structure is shown in Fig 1.0 below (Image Credit: [Garry Taylor from morioh.com](https://morioh.com/p/93609ebc63df)). 
The model I designed follows the general structure of the model proposed in ["UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS"](https://arxiv.org/pdf/1511.06434.pdf) by Alec Radford, A., Metz, L., and Chintala, S..
While the model structure is not very complicated, it is quite versatile. 
I trained this model on both the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) and the [CelebA dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) to demonstrate the model's capability of generating images with simple and complex features. 
The generated images are shown in the [Result](#result) section.
<figure>
<img src="https://github.com/xSegFaultx/FaceGeneration/raw/master/images/fig1.0.png" alt="dcgan structure">
<figcaption align = "center"><b>Fig.1.0 - Structure of the DCGAN model </b></figcaption>
</figure>

# DCGAN Model
## Generator
The generator takes a vector of random noise as input and outputs an image.
The goal of the generator is to generate images as realistic as possible (the generated image should be as close to the distribution of the training image as possible) so that it could "fool" the discriminator.
The structure of the generator is shown in Fig 2.0.
<figure>
<img src="https://github.com/xSegFaultx/FaceGeneration/raw/master/images/fig2.0.png" alt="Generator">
<figcaption align = "center"><b>Fig.2.0 - Structure of the Generator </b></figcaption>
</figure>


Here, Z is the random noise vector input and it is first passed to a fully connected layer and then reshaped into a 3D tensor. 
The FC layer and the reshaping step project the input to a small spatial extent convolutional representation with many feature maps. 
This stack of feature maps is then passed to a series of transpose convolutional layers (also known as fractionally-strided convolutions).
These transpose convolutional layers are quite the opposite of traditional convolutional layers. 
The traditional convolution layer tries to downsampling the input and extract features from the input while the transpose convolution layer tries to upsampling the input and add features to the input. 
Fig 2.1 shows how the transpose convolution operation works. 
<figure>
<img src="https://github.com/xSegFaultx/FaceGeneration/raw/master/images/fig2.1.gif" alt="transpose convolution">
<figcaption align = "center"><b>Fig.2.1 - Transpose Convolution <a href="https://github.com/vdumoulin/conv_arithmetic">Image Credit</a> </b></figcaption>
</figure>


The input (blue cells) is padded with zero values (white cells) and the kernel performs convolution operation on this padded feature map. 
By training the network, the kernel needs to learn to how "fill in" (interpreted) these zero values and thus upsampled the input. 
In this project, I used 5 transpose convolution layers followed by batch normalization and leaky relu activation after each layer (the output layer does not use batch normalization and uses tanh activation) to achieve the best result.

## Discriminator
Two types of images will be fed to the discriminator:
1. Real Image: Image from the training dataset
2. Fake Image: Image generated by the generator

The discriminator's job is to tell the differences between the real and the fake images. 
This can be viewed as a binary classification task where the discriminator needs to classifier the real and fake images into two separate classes. 
Therefore, I designed this simple CNN classifier as shown in Fig 3.0.
<figure>
<img src="https://github.com/xSegFaultx/FaceGeneration/raw/master/images/fig3.0.png" alt="Discriminator">
<figcaption align = "center"><b>Fig.3.0 - Structure of the Discriminator </b></figcaption>
</figure>


Usually, in many traditional CNN models, pooling layers are used to downsample the input. 
However, in this discriminator, I removed all the pooling layers and let the stride convolutions do all the downsampling work.
This allows the convolution layer to learn its own spatial downsampling.
This technic is also used in the generator.

## Loss Calculation
The loss function for the discriminator is quite straightforward. 
Since we are training for a binary classification task, we can use the cross entropy loss function. 
Let's make the following assumption:
```
CEL(x, y) ---> cross entropy loss between x and y
G(x) ---> output of the generator with input x
D(x) ---> output of the discriminator with input x
oneslike(x) ---> array of 1 which has the same shape as x
zeroslike(x) ---> array of 0 which has the same shape as x
```
Let's also assume the real images should have label 1 and fake images should have label 0. \
Then the loss for the discriminator chould be written as:
```
real_image_loss = CEL(D(real_image), oneslike(D(real_image)))
fake_image_loss = CEL(D(G(x)), zeroslike(D(G(x))))
discriminator_loss = real_image_loss + fake_image_loss
```
The loss for the generator will be a bit tricky. 
The goal of training the generator is to let the generator generate images as real as possible so that it could "fool" the discriminator. 
Therefore, in order to improve the generator, the discriminator should tell the generator how fake the fake images are. 
Therefore, the loss function for the generator could be written as:
```
generator_loss = CEL(D(G(x)), oneslike(D(G(x))))
```
This function calculates the distance between the real images and the image generated by the generator by using the discriminator. 
Then, this distance is propagated back to the generator so that it can be improved.


# Result
The DCGAN is trained on two datasets: MNIST and CelebA. 
The model trained on the MNIST dataset is able to generate handwritten digits and the one trained on the CelebA dataset is able to generate realistic human faces. 
Results for both models are shown below in Fig 4.0.
<figure>
<img src="https://github.com/xSegFaultx/FaceGeneration/raw/master/images/fig4.0a.png" alt="test result 1">
<img src="https://github.com/xSegFaultx/FaceGeneration/raw/master/images/fig4.0b.png" alt="test result 2">
<figcaption align = "center"><b>Fig.4.0 - Top: Hand-written digits generated by the DCGAN model Bottom: Human Faces generated by the DCGAN model </b></figcaption>
</figure>


Even though the model performs quite well on these two dataset, the model could still be improved in serval ways. 
First, the image resolution generated by the model could be higher. 
This could be achieved by implementing a deeper DCGAN model. 
Also, since we already implemented a DCGAN, we could extend it to CycleGAN models which allow us to perform tasks like object transfer, style transfer and many more.

# WARNING
This is a project from Udacity's ["Deep Learning Nanodegree"](https://www.udacity.com/course/deep-learning-nanodegree--nd101). I hope my code can help you with your project (if you are working on the same project as this one) but please do not copy my code and please follow [Udacity Honor Code](https://www.udacity.com/legal/community-guidelines) when you are doing your project.

